# VAE & GAN on CIFAR-10 Dataset

This notebook demonstrates the training and comparison of two generative models â€” a **Variational Autoencoder (VAE)** and a **Wasserstein GAN with Gradient Penalty (WGAN-GP)** â€” on the [CIFAR-10 PNGs in Folders dataset](https://www.kaggle.com/datasets/swaroopkml/cifar10-pngs-in-folders).

Both models are trained to learn and generate 32Ã—32 RGB images across 2 classes  cat and dog.

---

## ğŸ“¦ Dataset

- **Name:** CIFAR-10 PNGs in Folders
- **Source:** [Kaggle - swaroopkml](https://www.kaggle.com/datasets/swaroopkml/cifar10-pngs-in-folders)
- **Structure:**
cifar10/train/
â”œâ”€â”€ ...
â”œâ”€â”€ cat/
â”œâ”€â”€ ...
â”œâ”€â”€ dog/
â””â”€â”€ ...


---

## ğŸ§  Models Implemented

### ğŸ”¹ 1. Variational Autoencoder (VAE)
- **Encoder:** 3-layer CNN encodes image into latent distribution (mean and log variance).
- **Decoder:** 3-layer deconv net reconstructs image from latent code.
- **Loss:** Reconstruction Loss + KL Divergence.

### ğŸ”¹ 2. WGAN-GP (Wasserstein GAN with Gradient Penalty)
- **Generator:** Upsampling from 128-D noise vector to 32Ã—32 RGB.
- **Discriminator (Critic):** CNN with spectral normalization.
- **Loss:** Wasserstein Loss + Gradient Penalty.

---

## ğŸ“Š Visualizations

### âœ… Training Loss Curve
Track of critic and generator losses over epochs:
Combined reconstruction and KL loss across epochs:

![Loss curve](screenshots/loss_curves.png)


---

### ğŸ–¼ï¸ Final VAE Output (Reconstructions)

Comparison of original CIFAR-10 images with VAE reconstructions:

![VAE Output](screenshots/vae_output.png)

---

### ğŸ–¼ï¸ Final GAN Output (Generated Samples)

Randomly generated CIFAR-10-like samples using WGAN-GP:

![GAN Output](screenshots/gan_output.png)

---

## ğŸ›  Training Configuration

| Parameter        | Value        |
|------------------|--------------|
| Epochs           | 400          |
| Batch Size       | 128          |
| Image Size       | 32x32        |
| Latent Dim (GAN) | 128          |
| Latent Dim (VAE) | 100          |
| Optimizer        | Adam         |
| Dataset Format   | RGB (3 channels) |

---

## ğŸš€ How to Run

1. Download and extract [this dataset](https://www.kaggle.com/datasets/swaroopkml/cifar10-pngs-in-folders).
2. Clone this repo or run the notebook in your environment.
3. Make sure PyTorch, torchvision, matplotlib, and progressbar2 are installed.
4. Execute all cells in sequence for full training.

---

## ğŸ“¦ Requirements

```bash

pip install torch torchvision matplotlib progressbar2

```

## ğŸ“Œ Notes
GAN may produce better sample quality but requires careful training.

VAE tends to blur outputs but has strong latent representation learning.

You can adjust the latent dimensions and loss coefficients to tune performance.

