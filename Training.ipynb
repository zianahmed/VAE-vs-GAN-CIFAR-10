{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":283795,"sourceType":"datasetVersion","datasetId":118250}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries Import","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, utils","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"# Define image transformations: resize, tensor conversion, and normalization\ntransform = transforms.Compose([\n    transforms.Resize((32, 32)),\n    transforms.ToTensor(),\n    # Normalize to [-1, 1] for Tanh activation in generator and decoder\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Load the CIFAR-10 dataset (all classes are loaded initially)\ndataset_path = \"/kaggle/input/cifar10-pngs-in-folders/cifar10/train\"\ndataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n\n# Filter dataset to only include \"cat\" and \"dog\" classes\nselected_classes = ['cat', 'dog']\nselected_indices = [\n    i for i, (path, label) in enumerate(dataset.samples)\n    if dataset.classes[label] in selected_classes\n]\nsubset_dataset = Subset(dataset, selected_indices)\nprint(f\"Total samples for cat & dog: {len(subset_dataset)}\")\n\n# Create DataLoader\nbatch_size = 64\ndataloader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GAN","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, latent_dim=100, feature_map_size=64):\n        super(Generator, self).__init__()\n        self.net = nn.Sequential(\n            # Input: latent_dim x 1 x 1\n            nn.ConvTranspose2d(latent_dim, feature_map_size * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(feature_map_size * 8),\n            nn.ReLU(True),\n            # State: (feature_map_size*8) x 4 x 4\n            nn.ConvTranspose2d(feature_map_size * 8, feature_map_size * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(feature_map_size * 4),\n            nn.ReLU(True),\n            # State: (feature_map_size*4) x 8 x 8\n            nn.ConvTranspose2d(feature_map_size * 4, feature_map_size * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(feature_map_size * 2),\n            nn.ReLU(True),\n            # State: (feature_map_size*2) x 16 x 16\n            nn.ConvTranspose2d(feature_map_size * 2, feature_map_size, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(feature_map_size),\n            nn.ReLU(True),\n            # State: (feature_map_size) x 32 x 32\n            nn.ConvTranspose2d(feature_map_size, 3, 3, 1, 1, bias=False),\n            nn.Tanh()  # Output in range [-1, 1]\n        )\n        \n    def forward(self, z):\n        return self.net(z)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## MiniBatch Discrimination","metadata":{}},{"cell_type":"code","source":"class MinibatchDiscrimination(nn.Module):\n    def __init__(self, in_features, out_features, kernel_dim):\n        super(MinibatchDiscrimination, self).__init__()\n        self.out_features = out_features\n        self.kernel_dim = kernel_dim\n        self.T = nn.Parameter(torch.Tensor(in_features, out_features * kernel_dim))\n        nn.init.normal_(self.T, 0, 1)\n    \n    def forward(self, x):\n        # x shape: (batch, in_features)\n        M = x.matmul(self.T)  # shape: (batch, out_features * kernel_dim)\n        M = M.view(-1, self.out_features, self.kernel_dim)  # shape: (batch, out_features, kernel_dim)\n        # Compute L1 distances between samples in minibatch\n        out = []\n        for i in range(x.size(0)):\n            diff = torch.abs(M[i].unsqueeze(0) - M)  # shape: (batch, out_features, kernel_dim)\n            exp = torch.exp(-torch.sum(diff, dim=2))  # shape: (batch, out_features)\n            # Subtract self distance\n            out.append(torch.sum(exp, dim=0) - 1)\n        out = torch.stack(out)  # shape: (batch, out_features)\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Siamese Discriminator","metadata":{}},{"cell_type":"code","source":"class SiameseDiscriminator(nn.Module):\n    def __init__(self, feature_map_size=64, use_minibatch_discrimination=True):\n        super(SiameseDiscriminator, self).__init__()\n        # Shared feature extractor for both inputs\n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(3, feature_map_size, 4, 2, 1),  # 32x32 -> 16x16\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(feature_map_size, feature_map_size * 2, 4, 2, 1),  # 16x16 -> 8x8\n            nn.BatchNorm2d(feature_map_size * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(feature_map_size * 2, feature_map_size * 4, 4, 2, 1),  # 8x8 -> 4x4\n            nn.BatchNorm2d(feature_map_size * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n        self.flatten = nn.Flatten()\n        # Feature dimension: channels * 4 * 4\n        self.feature_dim = feature_map_size * 4 * 4 * 4\n        \n        self.use_minibatch_discrimination = use_minibatch_discrimination\n        if use_minibatch_discrimination:\n            self.mbd = MinibatchDiscrimination(self.feature_dim, 100, 5)  # hyperparameters chosen arbitrarily\n        \n        # After processing each image, we combine the features from the pair\n        # If using MBD, concatenate extra features\n        fc_input_dim = self.feature_dim + (100 if use_minibatch_discrimination else 0)\n        self.fc = nn.Sequential(\n            nn.Linear(fc_input_dim, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1)  # Outputs a single similarity score\n        )\n        \n    def forward_once(self, x):\n        features = self.feature_extractor(x)\n        features = self.flatten(features)\n        if self.use_minibatch_discrimination:\n            mbd_features = self.mbd(features)\n            features = torch.cat([features, mbd_features], dim=1)\n        return features\n        \n    def forward(self, real, fake):\n        # Process real and fake images separately using the shared network\n        real_feat = self.forward_once(real)\n        fake_feat = self.forward_once(fake)\n        # Compute the absolute difference between feature representations\n        diff = torch.abs(real_feat - fake_feat)\n        # The fully connected layers output a similarity score\n        score = self.fc(diff)\n        return score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VAE","metadata":{}},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self, latent_dim=128, feature_map_size=64):\n        super(VAE, self).__init__()\n        # Encoder: 3x32x32 image -> latent space\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, feature_map_size, 4, 2, 1),  # 32x32 -> 16x16\n            nn.ReLU(),\n            nn.Conv2d(feature_map_size, feature_map_size * 2, 4, 2, 1),  # 16x16 -> 8x8\n            nn.BatchNorm2d(feature_map_size * 2),\n            nn.ReLU(),\n            nn.Conv2d(feature_map_size * 2, feature_map_size * 4, 4, 2, 1),  # 8x8 -> 4x4\n            nn.BatchNorm2d(feature_map_size * 4),\n            nn.ReLU(),\n        )\n        self.flatten = nn.Flatten()\n        self.feature_dim = feature_map_size * 4 * 4 * 4  # channels*4*4\n        self.fc_mu = nn.Linear(self.feature_dim, latent_dim)\n        self.fc_logvar = nn.Linear(self.feature_dim, latent_dim)\n        \n        # Decoder: latent vector -> reconstructed image\n        self.decoder_input = nn.Linear(latent_dim, self.feature_dim)\n        self.decoder = nn.Sequential(\n            nn.Unflatten(1, (feature_map_size * 4, 4, 4)),\n            nn.ConvTranspose2d(feature_map_size * 4, feature_map_size * 2, 4, 2, 1),  # 4x4 -> 8x8\n            nn.BatchNorm2d(feature_map_size * 2),\n            nn.ReLU(),\n            nn.ConvTranspose2d(feature_map_size * 2, feature_map_size, 4, 2, 1),  # 8x8 -> 16x16\n            nn.BatchNorm2d(feature_map_size),\n            nn.ReLU(),\n            nn.ConvTranspose2d(feature_map_size, 3, 4, 2, 1),  # 16x16 -> 32x32\n            nn.Tanh()  # Outputs in range [-1, 1]\n        )\n    \n    def encode(self, x):\n        x = self.encoder(x)\n        x = self.flatten(x)\n        mu = self.fc_mu(x)\n        logvar = self.fc_logvar(x)\n        return mu, logvar\n        \n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n        \n    def decode(self, z):\n        x = self.decoder_input(z)\n        x = self.decoder(x)\n        return x\n        \n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        recon = self.decode(z)\n        return recon, mu, logvar\n\n# Loss function for VAE\ndef vae_loss(recon, x, mu, logvar):\n    recon_loss = nn.functional.mse_loss(recon, x, reduction='sum')\n    # KL divergence loss\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return recon_loss + kl_loss, recon_loss, kl_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyper Parameters","metadata":{}},{"cell_type":"code","source":"# Aggressive Hyperparameter Tuning for GAN and VAE\n\n# Increase total epochs for longer, more fine-grained training\nnum_epochs = 250             # For GAN training\nnum_epochs_vae = 250         # For VAE training\n\n# For GAN:\nlatent_dim = 100             # Remains unchanged for GAN\n# Use a relatively moderate generator learning rate,\n# but drastically reduce the discriminator's learning rate to slow its training,\n# which gives the generator a chance to catch up.\nlr_gen = 1e-4                # Generator learning rate\nlr_disc = 1e-5               # Discriminator learning rate (aggressively low)\n\n# For VAE:\n# Increase latent dimension to capture more details\nvae_latent_dim = 512         # Aggressively increased latent dimension for sharper details\nlr_vae = 1e-5                # Lower learning rate for VAE for slow, careful optimization\n\n# Initialize GAN components (assumes Generator and SiameseDiscriminator are defined)\ngen = Generator(latent_dim=latent_dim).to(device)\ndisc = SiameseDiscriminator().to(device)\n\n# Initialize VAE (assumes VAE is defined)\nvae = VAE(latent_dim=vae_latent_dim).to(device)\n\n# Optimizers for each model with aggressive learning rates\noptimizerG = optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.5, 0.999))\noptimizerD = optim.Adam(disc.parameters(), lr=lr_disc, betas=(0.5, 0.999))\noptimizerVAE = optim.Adam(vae.parameters(), lr=lr_vae)\n\n# Optionally, you can use learning rate schedulers to decay the rates further over time:\nschedulerG = torch.optim.lr_scheduler.StepLR(optimizerG, step_size=50, gamma=0.5)\nschedulerD = torch.optim.lr_scheduler.StepLR(optimizerD, step_size=50, gamma=0.5)\nschedulerVAE = torch.optim.lr_scheduler.StepLR(optimizerVAE, step_size=50, gamma=0.5)\n\n# Aggressive VAE Loss Function:\n# Use L1 loss (which can yield sharper outputs) for reconstruction,\n# and significantly reduce the weight on KL divergence so the latent space doesn't overly constrain detail.\ndef vae_loss(recon, x, mu, logvar, kl_weight=0.01):\n    # L1 reconstruction loss encourages sharper reconstructions than MSE\n    recon_loss = nn.functional.l1_loss(recon, x, reduction='sum')\n    # Standard KL divergence term\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    total_loss = recon_loss + kl_weight * kl_loss\n    return total_loss, recon_loss, kl_loss\n\n# If you find that the generator is still lagging,\n# consider updating the generator more times per discriminator update.\n# For example, you could run two generator updates for every discriminator update\n# within your training loop.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## GAN","metadata":{}},{"cell_type":"code","source":"# GAN Training Cell\ngan_loss_history = {'G': [], 'D': []}\n\ngen.train()\ndisc.train()\nprint(\"Starting GAN training...\")\n\nfor epoch in range(num_epochs):\n    for i, (imgs, _) in enumerate(dataloader):\n        real_imgs = imgs.to(device)\n        current_batch = real_imgs.size(0)\n        \n        # ----------------\n        # Train Discriminator:\n        # ----------------\n        # Generate fake images\n        noise = torch.randn(current_batch, latent_dim, 1, 1, device=device)\n        fake_imgs = gen(noise)\n        \n        disc.zero_grad()\n        # The discriminator takes a pair (real, fake) and should output a high score.\n        score = disc(real_imgs, fake_imgs)\n        lossD = - torch.mean(score)  # maximize dissimilarity between real & fake\n        lossD.backward()\n        optimizerD.step()\n        \n        # ----------------\n        # Train Generator:\n        # ----------------\n        gen.zero_grad()\n        noise = torch.randn(current_batch, latent_dim, 1, 1, device=device)\n        fake_imgs = gen(noise)\n        score = disc(real_imgs, fake_imgs)\n        lossG = torch.mean(score)  # minimize similarity score so fakes look more like reals\n        lossG.backward()\n        optimizerG.step()\n        \n        # Log every 50 iterations\n        if i % 50 == 0:\n            print(f\"GAN Epoch [{epoch+1}/{num_epochs}] Batch {i}/{len(dataloader)}  Loss_D: {lossD.item():.4f}  Loss_G: {lossG.item():.4f}\")\n            gan_loss_history['G'].append(lossG.item())\n            gan_loss_history['D'].append(lossD.item())\n\n    # Optionally, save model checkpoints at the end of each epoch\n    torch.save(gen.state_dict(), f'/kaggle/working/gen_epoch_{epoch+1}.pt')\n    torch.save(disc.state_dict(), f'/kaggle/working/disc_epoch_{epoch+1}.pt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VAE","metadata":{}},{"cell_type":"code","source":"# VAE Training Cell\nvae_loss_history = {'total': [], 'recon': [], 'kl': []}\n\nvae.train()\nprint(\"\\nStarting VAE training...\")\n\nfor epoch in range(num_epochs_vae):\n    for i, (imgs, _) in enumerate(dataloader):\n        imgs = imgs.to(device)\n        vae.zero_grad()\n        recon, mu, logvar = vae(imgs)\n        loss, recon_loss, kl_loss = vae_loss(recon, imgs, mu, logvar)\n        loss.backward()\n        optimizerVAE.step()\n        \n        if i % 50 == 0:\n            print(f\"VAE Epoch [{epoch+1}/{num_epochs_vae}] Batch {i}/{len(dataloader)}  Loss: {loss.item():.4f}  Recon: {recon_loss.item():.4f}  KL: {kl_loss.item():.4f}\")\n            vae_loss_history['total'].append(loss.item())\n            vae_loss_history['recon'].append(recon_loss.item())\n            vae_loss_history['kl'].append(kl_loss.item())\n    \n    # Optionally, save the VAE model at the end of each epoch\n    torch.save(vae.state_dict(), f'/kaggle/working/vae_epoch_{epoch+1}.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:17:31.098046Z","iopub.execute_input":"2025-03-30T18:17:31.098405Z","iopub.status.idle":"2025-03-30T18:17:59.948772Z","shell.execute_reply.started":"2025-03-30T18:17:31.098379Z","shell.execute_reply":"2025-03-30T18:17:59.947746Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## GAN","metadata":{}},{"cell_type":"code","source":"gen.eval()\nwith torch.no_grad():\n    fixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n    fake_samples = gen(fixed_noise).detach().cpu()\n\n# Denormalize images from [-1,1] to [0,1]\nfake_samples = (fake_samples + 1) / 2\n\nplt.figure(figsize=(8, 8))\nplt.axis(\"off\")\nplt.title(\"GAN Generated Images\")\nplt.imshow(np.transpose(utils.make_grid(fake_samples, padding=2, normalize=True).cpu(), (1, 2, 0)))\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:13:16.490194Z","iopub.execute_input":"2025-03-30T18:13:16.490525Z","iopub.status.idle":"2025-03-30T18:13:17.032920Z","shell.execute_reply.started":"2025-03-30T18:13:16.490490Z","shell.execute_reply":"2025-03-30T18:13:17.031997Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VAE","metadata":{}},{"cell_type":"code","source":"vae.eval()\nwith torch.no_grad():\n    data_iter = iter(dataloader)\n    imgs, _ = next(data_iter)\n    imgs = imgs.to(device)\n    recon, _, _ = vae(imgs)\n    # Denormalize both original and reconstructed images\n    imgs_denorm = (imgs + 1) / 2\n    recon_denorm = (recon + 1) / 2\n\n# Display original vs. reconstructed images\nn = 8\nplt.figure(figsize=(16, 4))\nfor i in range(n):\n    # Original images on top row\n    plt.subplot(2, n, i + 1)\n    plt.imshow(np.transpose(imgs_denorm[i].cpu(), (1, 2, 0)))\n    plt.axis('off')\n    # Reconstructed images on bottom row\n    plt.subplot(2, n, i + 1 + n)\n    plt.imshow(np.transpose(recon_denorm[i].cpu(), (1, 2, 0)))\n    plt.axis('off')\nplt.suptitle(\"VAE: Original (top) vs. Reconstructed (bottom)\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:13:17.034060Z","iopub.execute_input":"2025-03-30T18:13:17.034375Z","iopub.status.idle":"2025-03-30T18:13:17.571241Z","shell.execute_reply.started":"2025-03-30T18:13:17.034347Z","shell.execute_reply":"2025-03-30T18:13:17.570078Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loss Curves","metadata":{}},{"cell_type":"code","source":"# GAN Loss curves\nplt.figure()\nplt.plot(gan_loss_history['G'], label='Generator Loss')\nplt.plot(gan_loss_history['D'], label='Discriminator Loss')\nplt.xlabel(\"Iterations (logged every 50 batches)\")\nplt.ylabel(\"Loss\")\nplt.title(\"GAN Training Loss\")\nplt.legend()\nplt.show()\n\n# VAE Loss curves\nplt.figure()\nplt.plot(vae_loss_history['total'], label='Total Loss')\nplt.plot(vae_loss_history['recon'], label='Reconstruction Loss')\nplt.plot(vae_loss_history['kl'], label='KL Divergence')\nplt.xlabel(\"Iterations (logged every 50 batches)\")\nplt.ylabel(\"Loss\")\nplt.title(\"VAE Training Loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:13:17.572563Z","iopub.execute_input":"2025-03-30T18:13:17.572947Z","iopub.status.idle":"2025-03-30T18:13:17.948455Z","shell.execute_reply.started":"2025-03-30T18:13:17.572908Z","shell.execute_reply":"2025-03-30T18:13:17.947533Z"}},"outputs":[],"execution_count":null}]}